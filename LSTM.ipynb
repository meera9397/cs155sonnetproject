{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Sequential Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Lambda\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# load in text \n",
    "raw_text = open(os.path.join(os.getcwd(), 'data_shakespeare/shakespeare.txt')).read()\n",
    "lines = [line.split() for line in raw_text.split('\\n\\n') if line.split()]\n",
    "\n",
    "# remove all unnecessary characters from the text\n",
    "raw_text2 = ''\n",
    "for line in lines:\n",
    "    obs_elem = []\n",
    "    for word in line:\n",
    "        word = re.sub(\"\\d+\", \"\", word)\n",
    "        if (word == \"\"):\n",
    "            continue\n",
    "        word = re.sub(r'[^-\\w\\']', '', word).lower()\n",
    "        raw_text2 += word + ' '\n",
    "            \n",
    "# create same-length strings \n",
    "length = 40\n",
    "# get list of all characters used in text\n",
    "chars = sorted(list(set(raw_text2)))\n",
    "# map characters to their numerical value\n",
    "mapping = dict((c, i) for i, c in enumerate(chars))\n",
    "vocab_size = len(mapping)\n",
    "\n",
    "# tokenize a 40 length sequence and the character\n",
    "# coming after it (the 40 length sequence will be \n",
    "# x and the 41st character will be y)\n",
    "X = []\n",
    "y = []\n",
    "for i in range(length, int(len(raw_text2)/2)):\n",
    "    # select sequence of tokens\n",
    "    seq = raw_text2[i-length:i+1]\n",
    "    # store\n",
    "    encoded_seq = [mapping[c] for c in list(seq)]\n",
    "    X.append(np.array(encoded_seq))\n",
    "    output = raw_text2[i + 1]\n",
    "    encoded_seq2 = mapping[output]\n",
    "    y.append(encoded_seq2)\n",
    "    \n",
    "# separate into input and output\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "beforepX = X\n",
    "# \n",
    "newsequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
    "X = np.array(newsequences)\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate poem from Sequential Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poem(model, chars):\n",
    "    start = np.random.randint(0, len(beforepX)-1)\n",
    "    int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "    pattern = list(beforepX[start])\n",
    "    # generate characters\n",
    "    poem = ''\n",
    "    for k in range(1400):\n",
    "        x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "        x = to_categorical(x, num_classes=vocab_size)\n",
    "        prediction = model.predict(x, verbose=0)\n",
    "        index = np.argmax(prediction)\n",
    "        result = int_to_char[index]\n",
    "        seq_in = [int_to_char[value] for value in pattern]\n",
    "        poem = poem + result\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "\n",
    "    punctuation_list = [',', '.', ':', '?']\n",
    "    punctuation_probs = [0.6, 0.1, 0.2, 0.1]\n",
    "    poem_format = poem.split()\n",
    "    for p in range(1, len(poem_format)):\n",
    "        if ((p - 1) % 50 == 0):\n",
    "            print(poem_format[p].capitalize() + ' ', end = '')\n",
    "        else:\n",
    "            print(poem_format[p], end = '')\n",
    "            if (p % 50 == 0 and p < len(poem_format) - 1):\n",
    "                print(np.random.choice(punctuation_list, p = punctuation_probs) + ' ', end = '')\n",
    "                print()\n",
    "            elif(p == len(poem_format) - 1):\n",
    "                print('.', end = '')\n",
    "            else:\n",
    "                print(' ', end = '')\n",
    "        \n",
    "            \n",
    "def summers_day_poem(model, chars):\n",
    "    test = \"shall i compare thee to a summers day sha\"\n",
    "    test = re.sub(\"\\d+\", \" \", test)\n",
    "    test = re.sub(r'[^-\\w\\']', ' ', test).lower()\n",
    "    pattern = [mapping[c] for c in list(test)]\n",
    "    int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "    # generate characters\n",
    "    poem = ''\n",
    "    for k in range(1400):\n",
    "        x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "        x = to_categorical(x, num_classes=vocab_size)\n",
    "        prediction = model.predict(x, verbose=0)\n",
    "        index = np.argmax(prediction)\n",
    "        result = int_to_char[index]\n",
    "        seq_in = [int_to_char[value] for value in pattern]\n",
    "        poem = poem + result\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "\n",
    "    poem_format = poem.split()\n",
    "    for p in range(1, len(poem_format)):\n",
    "        if ((p - 1) % 50 == 0):\n",
    "            print(poem_format[p].capitalize() + ' ', end = '')\n",
    "        else:\n",
    "            print(poem_format[p] + ' ', end = '')\n",
    "        if (p % 50 == 0):\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperature = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 200)               184000    \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 29)                5829      \n",
      "=================================================================\n",
      "Total params: 189,829\n",
      "Trainable params: 189,829\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/90\n",
      " - 131s - loss: 2.4162 - acc: 0.3039\n",
      "Epoch 2/90\n",
      " - 119s - loss: 2.0267 - acc: 0.4014\n",
      "Epoch 3/90\n",
      " - 107s - loss: 1.8837 - acc: 0.4376\n",
      "Epoch 4/90\n",
      " - 110s - loss: 1.7902 - acc: 0.4620\n",
      "Epoch 5/90\n",
      " - 113s - loss: 1.7152 - acc: 0.4810\n",
      "Epoch 6/90\n",
      " - 125s - loss: 1.6532 - acc: 0.4967\n",
      "Epoch 7/90\n",
      " - 123s - loss: 1.5989 - acc: 0.5096\n",
      "Epoch 8/90\n",
      " - 108s - loss: 1.5482 - acc: 0.5232\n",
      "Epoch 9/90\n",
      " - 107s - loss: 1.5021 - acc: 0.5365\n",
      "Epoch 10/90\n",
      " - 107s - loss: 1.4593 - acc: 0.5505\n",
      "Epoch 11/90\n",
      " - 108s - loss: 1.4096 - acc: 0.5628\n",
      "Epoch 12/90\n",
      " - 107s - loss: 1.3659 - acc: 0.5739\n",
      "Epoch 13/90\n",
      " - 109s - loss: 1.3209 - acc: 0.5872\n",
      "Epoch 14/90\n",
      " - 109s - loss: 1.2761 - acc: 0.6010\n",
      "Epoch 15/90\n",
      " - 109s - loss: 1.2319 - acc: 0.6122\n",
      "Epoch 16/90\n",
      " - 109s - loss: 1.1873 - acc: 0.6286\n",
      "Epoch 17/90\n",
      " - 110s - loss: 1.1459 - acc: 0.6398\n",
      "Epoch 18/90\n",
      " - 109s - loss: 1.2470 - acc: 0.6070\n",
      "Epoch 19/90\n",
      " - 108s - loss: 1.3890 - acc: 0.5740\n",
      "Epoch 20/90\n",
      " - 107s - loss: 1.4358 - acc: 0.5497\n",
      "Epoch 21/90\n",
      " - 110s - loss: 1.2257 - acc: 0.6127\n",
      "Epoch 22/90\n",
      " - 116s - loss: 1.1427 - acc: 0.6399\n",
      "Epoch 23/90\n",
      " - 108s - loss: 1.0725 - acc: 0.6626\n",
      "Epoch 24/90\n",
      " - 109s - loss: 1.0634 - acc: 0.6674\n",
      "Epoch 25/90\n",
      " - 108s - loss: 1.0550 - acc: 0.6666\n",
      "Epoch 26/90\n",
      " - 108s - loss: 1.0059 - acc: 0.6833\n",
      "Epoch 27/90\n",
      " - 107s - loss: 0.9741 - acc: 0.6923\n",
      "Epoch 28/90\n",
      " - 107s - loss: 0.9510 - acc: 0.7020\n",
      "Epoch 29/90\n",
      " - 105s - loss: 0.9248 - acc: 0.7081\n",
      "Epoch 30/90\n",
      " - 105s - loss: 0.8952 - acc: 0.7178\n",
      "Epoch 31/90\n",
      " - 108s - loss: 0.8679 - acc: 0.7282\n",
      "Epoch 32/90\n",
      " - 118s - loss: 0.8526 - acc: 0.7324\n",
      "Epoch 33/90\n",
      " - 115s - loss: 0.8141 - acc: 0.7434\n",
      "Epoch 34/90\n",
      " - 116s - loss: 0.7962 - acc: 0.7494\n",
      "Epoch 35/90\n",
      " - 117s - loss: 0.8324 - acc: 0.7387\n",
      "Epoch 36/90\n",
      " - 109s - loss: 0.7671 - acc: 0.7571\n",
      "Epoch 37/90\n",
      " - 109s - loss: 0.7060 - acc: 0.7804\n",
      "Epoch 38/90\n",
      " - 112s - loss: 0.7024 - acc: 0.7781\n",
      "Epoch 39/90\n",
      " - 112s - loss: 0.7928 - acc: 0.7471\n",
      "Epoch 40/90\n",
      " - 108s - loss: 0.6854 - acc: 0.7845\n",
      "Epoch 41/90\n",
      " - 108s - loss: 0.6538 - acc: 0.7962\n",
      "Epoch 42/90\n",
      " - 108s - loss: 0.6535 - acc: 0.7941\n",
      "Epoch 43/90\n",
      " - 105s - loss: 0.6341 - acc: 0.8011\n",
      "Epoch 44/90\n",
      " - 108s - loss: 0.6293 - acc: 0.7995\n",
      "Epoch 45/90\n",
      " - 108s - loss: 0.6126 - acc: 0.8060\n",
      "Epoch 46/90\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model1 = Sequential()\n",
    "model1.add(LSTM(200, input_shape=(X.shape[1], X.shape[2])))\n",
    "\n",
    "# adding temperature\n",
    "temp = 1.5\n",
    "model1.add(Lambda(lambda x : x /temp))\n",
    "model1.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "print(model1.summary())\n",
    "# compile model\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "\n",
    "model1.fit(X, y, epochs=90, verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_poem(model1, chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summers_day_poem(model1, chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperature = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(LSTM(200, input_shape=(X.shape[1], X.shape[2])))\n",
    "\n",
    "# adding temperature\n",
    "temp = 0.75\n",
    "model2.add(Lambda(lambda x : x /temp))\n",
    "model2.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "print(model2.summary())\n",
    "# compile model\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "\n",
    "model2.fit(X, y, epochs=90, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_poem(model2, chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summers_day_poem(model2, chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperature = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(LSTM(200, input_shape=(X.shape[1], X.shape[2])))\n",
    "\n",
    "# adding temperature\n",
    "temp = 0.25\n",
    "model3.add(Lambda(lambda x : x /temp))\n",
    "model3.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "print(model3.summary())\n",
    "# compile model\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "\n",
    "model3.fit(X, y, epochs=90, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_poem(model3, chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summers_day_poem(model3, chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
