{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Sequential Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91006\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Lambda, Dropout\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# load in text \n",
    "raw_text = open(os.path.join(os.getcwd(), 'data_shakespeare/shakespeare.txt')).read()\n",
    "lines = [line.split() for line in raw_text.split('\\n\\n') if line.split()]\n",
    "\n",
    "# remove all unnecessary characters from the text\n",
    "raw_text2 = ''\n",
    "for line in lines:\n",
    "    obs_elem = []\n",
    "    for word in line:\n",
    "        word = re.sub(\"\\d+\", \"\", word)\n",
    "        if (word == \"\"):\n",
    "            continue\n",
    "        word = re.sub(r'[^-\\w\\']', '', word).lower()\n",
    "        raw_text2 += word + ' '\n",
    "            \n",
    "# create same-length strings \n",
    "length = 40\n",
    "# get list of all characters used in text\n",
    "chars = sorted(list(set(raw_text2)))\n",
    "# map characters to their numerical value\n",
    "mapping = dict((c, i) for i, c in enumerate(chars))\n",
    "vocab_size = len(mapping)\n",
    "print(len(raw_text2))\n",
    "# tokenize a 40 length sequence and the character\n",
    "# coming after it (the 40 length sequence will be \n",
    "# x and the 41st character will be y)\n",
    "X = []\n",
    "y = []\n",
    "# using semi-redundant sequences to speed up training\n",
    "for i in range(length, len(raw_text2) - 1):\n",
    "    # select sequence of tokens\n",
    "    seq = raw_text2[i-length:i+1]\n",
    "    # store\n",
    "    encoded_seq = [mapping[c] for c in list(seq)]\n",
    "    X.append(np.array(encoded_seq))\n",
    "    output = raw_text2[i + 1]\n",
    "    encoded_seq2 = mapping[output]\n",
    "    y.append(encoded_seq2)\n",
    "    \n",
    "# separate into input and output\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "beforepX = X\n",
    "# converts x and y to binary class matrices (depending on \n",
    "# if the character is present in that sequence)\n",
    "newsequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
    "X = np.array(newsequences)\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate poem from Sequential Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source that I used to help generate this code:\n",
    "# https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "\n",
    "\n",
    "def generate_poem(model, chars):\n",
    "    ''' passing in a trained model and all the characters\n",
    "    that the model was trained on, generates a poem based\n",
    "    on model predictions '''\n",
    "    # get random initial line\n",
    "    start = np.random.randint(0, len(beforepX)-1)\n",
    "    # dictionary converting integers to characters\n",
    "    # (since we will generate predictions that are int sequences,\n",
    "    # we convert them to characters)\n",
    "    int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "    # get a random seed from the input data\n",
    "    pattern = list(beforepX[start])\n",
    "    # generate characters\n",
    "    # based on the input data, generate predictions \n",
    "    poem = ''\n",
    "    for k in range(700):\n",
    "        # get a prediction based on the pattern for what \n",
    "        # the next character will be \n",
    "        x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "        x = to_categorical(x, num_classes=vocab_size)\n",
    "        prediction = model.predict(x, verbose=0)\n",
    "        \n",
    "        # get prediction with highest probability \n",
    "        index = np.argmax(prediction)\n",
    "        result = int_to_char[index]\n",
    "        seq_in = [int_to_char[value] for value in pattern]\n",
    "        poem = poem + result\n",
    "        \n",
    "        # append that predicted pattern, remove first character \n",
    "        # from predicted pattern\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "    # randomly add punctuation to poem, print poem out\n",
    "    punctuation_list = [',', '.', ':', '?']\n",
    "    punctuation_probs = [0.6, 0.1, 0.2, 0.1]\n",
    "    poem_format = poem.split()\n",
    "    for p in range(1, len(poem_format)):\n",
    "        if ((p - 1) % 10 == 0):\n",
    "            print(poem_format[p].capitalize() + ' ', end = '')\n",
    "        else:\n",
    "            if (poem_format[p] == 'i'):\n",
    "                print(poem_format[p].capitalize() + ' ', end = '')\n",
    "            else:\n",
    "                print(poem_format[p], end = '')\n",
    "            if (p % 10 == 0 and p < len(poem_format) - 1):\n",
    "                print(np.random.choice(punctuation_list, p = punctuation_probs) + ' ', end = '')\n",
    "                print()\n",
    "            elif(p == len(poem_format) - 1):\n",
    "                print('.', end = '')\n",
    "            else:\n",
    "                print(' ', end = '')\n",
    "        \n",
    "            \n",
    "def summers_day_poem(model, chars):\n",
    "    ''' passing in a trained model and all the characters\n",
    "    that the model was trained on, generates a poem based\n",
    "    on model predictions -- the input is set to be shall\n",
    "    i compare thee to a summers day'''\n",
    "        \n",
    "    test = \"shall i compare thee to a summers day tho\"\n",
    "    test = re.sub(\"\\d+\", \" \", test)\n",
    "    test = re.sub(r'[^-\\w\\']', ' ', test).lower()\n",
    "    pattern = [mapping[c] for c in list(test)]\n",
    "    int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "    # generate characters\n",
    "    poem = ''\n",
    "    for k in range(700):\n",
    "        x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "        x = to_categorical(x, num_classes=vocab_size)\n",
    "        prediction = model.predict(x, verbose=0)\n",
    "        index = np.argmax(prediction)\n",
    "        result = int_to_char[index]\n",
    "        seq_in = [int_to_char[value] for value in pattern]\n",
    "        poem = poem + result\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "\n",
    "    punctuation_list = [',', '.', ':', '?']\n",
    "    punctuation_probs = [0.6, 0.1, 0.2, 0.1]\n",
    "    poem_format = poem.split()\n",
    "    for p in range(1, len(poem_format)):\n",
    "        if ((p - 1) % 10 == 0):\n",
    "            print(poem_format[p].capitalize() + ' ', end = '')\n",
    "        else:\n",
    "            if (poem_format[p] == 'i'):\n",
    "                print(poem_format[p].capitalize() + ' ', end = '')\n",
    "            else:\n",
    "                print(poem_format[p], end = '')\n",
    "            if (p % 10 == 0 and p < len(poem_format) - 1):\n",
    "                print(np.random.choice(punctuation_list, p = punctuation_probs) + ' ', end = '')\n",
    "                print()\n",
    "            elif(p == len(poem_format) - 1):\n",
    "                print('.', end = '')\n",
    "            else:\n",
    "                print(' ', end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperature = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 200)               184000    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 29)                5829      \n",
      "=================================================================\n",
      "Total params: 270,229\n",
      "Trainable params: 270,229\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      " - 263s - loss: 2.2224 - acc: 0.3478\n",
      "Epoch 2/200\n",
      " - 235s - loss: 1.9720 - acc: 0.4115\n",
      "Epoch 3/200\n",
      " - 216s - loss: 1.8737 - acc: 0.4366\n",
      "Epoch 4/200\n",
      " - 217s - loss: 1.8043 - acc: 0.4576\n",
      "Epoch 5/200\n",
      " - 218s - loss: 1.7499 - acc: 0.4703\n",
      "Epoch 6/200\n",
      " - 217s - loss: 1.7066 - acc: 0.4820\n",
      "Epoch 7/200\n",
      " - 219s - loss: 1.6682 - acc: 0.4923\n",
      "Epoch 8/200\n",
      " - 218s - loss: 1.6360 - acc: 0.4986\n",
      "Epoch 9/200\n",
      " - 217s - loss: 1.6071 - acc: 0.5071\n",
      "Epoch 10/200\n",
      " - 217s - loss: 1.5773 - acc: 0.5150\n",
      "Epoch 11/200\n",
      " - 219s - loss: 1.5550 - acc: 0.5213\n",
      "Epoch 12/200\n",
      " - 219s - loss: 1.5306 - acc: 0.5279\n",
      "Epoch 13/200\n",
      " - 219s - loss: 1.5110 - acc: 0.5338\n",
      "Epoch 14/200\n",
      " - 218s - loss: 1.4863 - acc: 0.5398\n",
      "Epoch 15/200\n",
      " - 218s - loss: 1.4634 - acc: 0.5455\n",
      "Epoch 16/200\n",
      " - 219s - loss: 1.4444 - acc: 0.5516\n",
      "Epoch 17/200\n",
      " - 220s - loss: 1.4198 - acc: 0.5578\n",
      "Epoch 18/200\n",
      " - 219s - loss: 1.4050 - acc: 0.5613\n",
      "Epoch 19/200\n",
      " - 219s - loss: 1.3832 - acc: 0.5689\n",
      "Epoch 20/200\n",
      " - 218s - loss: 1.3679 - acc: 0.5720\n",
      "Epoch 21/200\n",
      " - 218s - loss: 1.3499 - acc: 0.5777\n",
      "Epoch 22/200\n",
      " - 220s - loss: 1.3322 - acc: 0.5825\n",
      "Epoch 23/200\n",
      " - 218s - loss: 1.3152 - acc: 0.5879\n",
      "Epoch 24/200\n",
      " - 219s - loss: 1.2962 - acc: 0.5931\n",
      "Epoch 25/200\n",
      " - 219s - loss: 1.2863 - acc: 0.5958\n",
      "Epoch 26/200\n",
      " - 219s - loss: 1.2708 - acc: 0.6004\n",
      "Epoch 27/200\n",
      " - 218s - loss: 1.2561 - acc: 0.6049\n",
      "Epoch 28/200\n",
      " - 221s - loss: 1.2410 - acc: 0.6081\n",
      "Epoch 29/200\n",
      " - 218s - loss: 1.2336 - acc: 0.6116\n",
      "Epoch 30/200\n",
      " - 218s - loss: 1.2156 - acc: 0.6157\n",
      "Epoch 31/200\n",
      " - 216s - loss: 1.2050 - acc: 0.6189\n",
      "Epoch 32/200\n",
      " - 627s - loss: 1.1958 - acc: 0.6217\n",
      "Epoch 33/200\n",
      " - 680s - loss: 1.1841 - acc: 0.6254\n",
      "Epoch 34/200\n",
      " - 683s - loss: 1.1738 - acc: 0.6288\n",
      "Epoch 35/200\n",
      " - 683s - loss: 1.1645 - acc: 0.6316\n",
      "Epoch 36/200\n",
      " - 679s - loss: 1.1562 - acc: 0.6330\n",
      "Epoch 37/200\n",
      " - 682s - loss: 1.1500 - acc: 0.6344\n",
      "Epoch 38/200\n",
      " - 682s - loss: 1.1371 - acc: 0.6396\n",
      "Epoch 39/200\n",
      " - 679s - loss: 1.1309 - acc: 0.6414\n",
      "Epoch 40/200\n",
      " - 648s - loss: 1.1220 - acc: 0.6436\n",
      "Epoch 41/200\n",
      " - 220s - loss: 1.1121 - acc: 0.6467\n",
      "Epoch 42/200\n",
      " - 218s - loss: 1.0975 - acc: 0.6500\n",
      "Epoch 43/200\n",
      " - 218s - loss: 1.0979 - acc: 0.6504\n",
      "Epoch 44/200\n",
      " - 219s - loss: 1.0917 - acc: 0.6521\n",
      "Epoch 45/200\n",
      " - 220s - loss: 1.0869 - acc: 0.6542\n",
      "Epoch 46/200\n",
      " - 220s - loss: 1.0767 - acc: 0.6571\n",
      "Epoch 47/200\n",
      " - 218s - loss: 1.0695 - acc: 0.6600\n",
      "Epoch 48/200\n",
      " - 218s - loss: 1.0629 - acc: 0.6621\n",
      "Epoch 49/200\n",
      " - 218s - loss: 1.0552 - acc: 0.6639\n",
      "Epoch 50/200\n",
      " - 219s - loss: 1.0486 - acc: 0.6658\n",
      "Epoch 51/200\n",
      " - 219s - loss: 1.0461 - acc: 0.6677\n",
      "Epoch 52/200\n",
      " - 218s - loss: 1.0378 - acc: 0.6688\n",
      "Epoch 53/200\n",
      " - 219s - loss: 1.0337 - acc: 0.6717\n",
      "Epoch 54/200\n",
      " - 217s - loss: 1.0242 - acc: 0.6717\n",
      "Epoch 55/200\n",
      " - 217s - loss: 1.0226 - acc: 0.6738\n",
      "Epoch 56/200\n",
      " - 219s - loss: 1.0114 - acc: 0.6774\n",
      "Epoch 57/200\n",
      " - 220s - loss: 1.0111 - acc: 0.6767\n",
      "Epoch 58/200\n",
      " - 217s - loss: 1.0054 - acc: 0.6788\n",
      "Epoch 59/200\n",
      " - 218s - loss: 1.0014 - acc: 0.6793\n",
      "Epoch 60/200\n",
      " - 218s - loss: 0.9948 - acc: 0.6830\n",
      "Epoch 61/200\n",
      " - 219s - loss: 0.9879 - acc: 0.6834\n",
      "Epoch 62/200\n",
      " - 219s - loss: 0.9904 - acc: 0.6837\n",
      "Epoch 63/200\n",
      " - 218s - loss: 0.9786 - acc: 0.6887\n",
      "Epoch 64/200\n",
      " - 218s - loss: 0.9770 - acc: 0.6890\n",
      "Epoch 65/200\n",
      " - 218s - loss: 0.9750 - acc: 0.6889\n",
      "Epoch 66/200\n",
      " - 218s - loss: 0.9719 - acc: 0.6897\n",
      "Epoch 67/200\n",
      " - 217s - loss: 0.9584 - acc: 0.6932\n",
      "Epoch 68/200\n",
      " - 217s - loss: 0.9625 - acc: 0.6926\n",
      "Epoch 69/200\n",
      " - 220s - loss: 0.9527 - acc: 0.6953\n",
      "Epoch 70/200\n",
      " - 217s - loss: 0.9505 - acc: 0.6972\n",
      "Epoch 71/200\n",
      " - 217s - loss: 0.9459 - acc: 0.6966\n",
      "Epoch 72/200\n",
      " - 218s - loss: 0.9441 - acc: 0.6979\n",
      "Epoch 73/200\n",
      " - 217s - loss: 0.9377 - acc: 0.7010\n",
      "Epoch 74/200\n",
      " - 218s - loss: 0.9348 - acc: 0.6985\n",
      "Epoch 75/200\n",
      " - 217s - loss: 0.9317 - acc: 0.7028\n",
      "Epoch 76/200\n",
      " - 217s - loss: 0.9346 - acc: 0.7013\n",
      "Epoch 77/200\n",
      " - 218s - loss: 0.9249 - acc: 0.7036\n",
      "Epoch 78/200\n",
      " - 220s - loss: 0.9192 - acc: 0.7038\n",
      "Epoch 79/200\n",
      " - 217s - loss: 0.9140 - acc: 0.7058\n",
      "Epoch 80/200\n",
      " - 218s - loss: 0.9107 - acc: 0.7080\n",
      "Epoch 81/200\n",
      " - 218s - loss: 0.9097 - acc: 0.7079\n",
      "Epoch 82/200\n",
      " - 222s - loss: 0.9088 - acc: 0.7085\n",
      "Epoch 83/200\n",
      " - 219s - loss: 0.9064 - acc: 0.7103\n",
      "Epoch 84/200\n",
      " - 217s - loss: 0.8997 - acc: 0.7113\n",
      "Epoch 85/200\n",
      " - 218s - loss: 0.8982 - acc: 0.7120\n",
      "Epoch 86/200\n",
      " - 219s - loss: 0.8983 - acc: 0.7126\n",
      "Epoch 87/200\n",
      " - 218s - loss: 0.8923 - acc: 0.7136\n",
      "Epoch 88/200\n",
      " - 218s - loss: 0.8887 - acc: 0.7147\n",
      "Epoch 89/200\n",
      " - 218s - loss: 0.8855 - acc: 0.7145\n",
      "Epoch 90/200\n",
      " - 218s - loss: 0.8805 - acc: 0.7180\n",
      "Epoch 91/200\n",
      " - 220s - loss: 0.8803 - acc: 0.7176\n",
      "Epoch 92/200\n",
      " - 217s - loss: 0.8768 - acc: 0.7186\n",
      "Epoch 93/200\n",
      " - 217s - loss: 0.8788 - acc: 0.7181\n",
      "Epoch 94/200\n",
      " - 217s - loss: 0.8723 - acc: 0.7195\n",
      "Epoch 95/200\n",
      " - 218s - loss: 0.8712 - acc: 0.7182\n",
      "Epoch 96/200\n",
      " - 218s - loss: 0.8674 - acc: 0.7213\n",
      "Epoch 97/200\n",
      " - 219s - loss: 0.8626 - acc: 0.7231\n",
      "Epoch 98/200\n",
      " - 217s - loss: 0.8602 - acc: 0.7232\n",
      "Epoch 99/200\n",
      " - 218s - loss: 0.8594 - acc: 0.7239\n",
      "Epoch 100/200\n",
      " - 220s - loss: 0.8541 - acc: 0.7257\n",
      "Epoch 101/200\n",
      " - 217s - loss: 0.8550 - acc: 0.7249\n",
      "Epoch 102/200\n",
      " - 219s - loss: 0.8563 - acc: 0.7246\n",
      "Epoch 103/200\n",
      " - 216s - loss: 0.8519 - acc: 0.7249\n",
      "Epoch 104/200\n",
      " - 216s - loss: 0.8414 - acc: 0.7280\n",
      "Epoch 105/200\n",
      " - 219s - loss: 0.8406 - acc: 0.7291\n",
      "Epoch 106/200\n",
      " - 217s - loss: 0.8420 - acc: 0.7291\n",
      "Epoch 107/200\n",
      " - 217s - loss: 0.8375 - acc: 0.7295\n",
      "Epoch 108/200\n",
      " - 218s - loss: 0.8394 - acc: 0.7301\n",
      "Epoch 109/200\n",
      " - 217s - loss: 0.8363 - acc: 0.7306\n",
      "Epoch 110/200\n",
      " - 219s - loss: 0.8303 - acc: 0.7313\n",
      "Epoch 111/200\n",
      " - 218s - loss: 0.8302 - acc: 0.7323\n",
      "Epoch 112/200\n",
      " - 217s - loss: 0.8250 - acc: 0.7351\n",
      "Epoch 113/200\n",
      " - 218s - loss: 0.8277 - acc: 0.7319\n",
      "Epoch 114/200\n",
      " - 217s - loss: 0.8229 - acc: 0.7337\n",
      "Epoch 115/200\n",
      " - 221s - loss: 0.8197 - acc: 0.7356\n",
      "Epoch 116/200\n",
      " - 219s - loss: 0.8224 - acc: 0.7353\n",
      "Epoch 117/200\n",
      " - 218s - loss: 0.8191 - acc: 0.7366\n",
      "Epoch 118/200\n",
      " - 217s - loss: 0.8192 - acc: 0.7358\n",
      "Epoch 119/200\n",
      " - 218s - loss: 0.8139 - acc: 0.7385\n",
      "Epoch 120/200\n",
      " - 220s - loss: 0.8112 - acc: 0.7372\n",
      "Epoch 121/200\n",
      " - 219s - loss: 0.8113 - acc: 0.7382\n",
      "Epoch 122/200\n",
      " - 219s - loss: 0.8075 - acc: 0.7403\n",
      "Epoch 123/200\n",
      " - 219s - loss: 0.8037 - acc: 0.7400\n",
      "Epoch 124/200\n",
      " - 218s - loss: 0.8040 - acc: 0.7414\n",
      "Epoch 125/200\n",
      " - 219s - loss: 0.8040 - acc: 0.7400\n",
      "Epoch 126/200\n",
      " - 221s - loss: 0.7991 - acc: 0.7420\n",
      "Epoch 127/200\n",
      " - 220s - loss: 0.7974 - acc: 0.7416\n",
      "Epoch 128/200\n",
      " - 219s - loss: 0.7966 - acc: 0.7422\n",
      "Epoch 129/200\n",
      " - 218s - loss: 0.7952 - acc: 0.7428\n",
      "Epoch 130/200\n",
      " - 218s - loss: 0.7947 - acc: 0.7438\n",
      "Epoch 131/200\n",
      " - 217s - loss: 0.7911 - acc: 0.7447\n",
      "Epoch 132/200\n",
      " - 217s - loss: 0.7899 - acc: 0.7434\n",
      "Epoch 133/200\n",
      " - 217s - loss: 0.7921 - acc: 0.7449\n",
      "Epoch 134/200\n",
      " - 220s - loss: 0.7845 - acc: 0.7481\n",
      "Epoch 135/200\n",
      " - 222s - loss: 0.7838 - acc: 0.7459\n",
      "Epoch 136/200\n",
      " - 217s - loss: 0.7910 - acc: 0.7459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/200\n",
      " - 236s - loss: 0.7837 - acc: 0.7475\n",
      "Epoch 138/200\n",
      " - 258s - loss: 0.7790 - acc: 0.7483\n",
      "Epoch 139/200\n",
      " - 262s - loss: 0.7783 - acc: 0.7490\n",
      "Epoch 140/200\n",
      " - 257s - loss: 0.7755 - acc: 0.7490\n",
      "Epoch 141/200\n",
      " - 280s - loss: 0.7767 - acc: 0.7480\n",
      "Epoch 142/200\n",
      " - 218s - loss: 0.7772 - acc: 0.7479\n",
      "Epoch 143/200\n",
      " - 237s - loss: 0.7716 - acc: 0.7512\n",
      "Epoch 144/200\n",
      " - 224s - loss: 0.7689 - acc: 0.7502\n",
      "Epoch 145/200\n",
      " - 203s - loss: 0.7661 - acc: 0.7518\n",
      "Epoch 146/200\n",
      " - 204s - loss: 0.7686 - acc: 0.7517\n",
      "Epoch 147/200\n",
      " - 203s - loss: 0.7639 - acc: 0.7526\n",
      "Epoch 148/200\n",
      " - 204s - loss: 0.7619 - acc: 0.7520\n",
      "Epoch 149/200\n",
      " - 206s - loss: 0.7599 - acc: 0.7531\n",
      "Epoch 150/200\n",
      " - 205s - loss: 0.7621 - acc: 0.7524\n",
      "Epoch 151/200\n",
      " - 205s - loss: 0.7581 - acc: 0.7546\n",
      "Epoch 152/200\n",
      " - 204s - loss: 0.7606 - acc: 0.7530\n",
      "Epoch 153/200\n",
      " - 206s - loss: 0.7600 - acc: 0.7547\n",
      "Epoch 154/200\n",
      " - 206s - loss: 0.7569 - acc: 0.7554\n",
      "Epoch 155/200\n",
      " - 205s - loss: 0.7556 - acc: 0.7567\n",
      "Epoch 156/200\n",
      " - 204s - loss: 0.7513 - acc: 0.7560\n",
      "Epoch 157/200\n",
      " - 204s - loss: 0.7560 - acc: 0.7552\n",
      "Epoch 158/200\n",
      " - 205s - loss: 0.7458 - acc: 0.7592\n",
      "Epoch 159/200\n",
      " - 207s - loss: 0.7516 - acc: 0.7567\n",
      "Epoch 160/200\n",
      " - 206s - loss: 0.7489 - acc: 0.7592\n",
      "Epoch 161/200\n",
      " - 203s - loss: 0.7440 - acc: 0.7586\n",
      "Epoch 162/200\n",
      " - 205s - loss: 0.7463 - acc: 0.7589\n",
      "Epoch 163/200\n",
      " - 204s - loss: 0.7406 - acc: 0.7600\n",
      "Epoch 164/200\n",
      " - 205s - loss: 0.7389 - acc: 0.7614\n",
      "Epoch 165/200\n",
      " - 208s - loss: 0.7380 - acc: 0.7619\n",
      "Epoch 166/200\n",
      " - 207s - loss: 0.7373 - acc: 0.7610\n",
      "Epoch 167/200\n",
      " - 206s - loss: 0.7369 - acc: 0.7602\n",
      "Epoch 168/200\n",
      " - 206s - loss: 0.7355 - acc: 0.7620\n",
      "Epoch 169/200\n",
      " - 205s - loss: 0.7370 - acc: 0.7622\n",
      "Epoch 170/200\n",
      " - 206s - loss: 0.7357 - acc: 0.7608\n",
      "Epoch 171/200\n",
      " - 204s - loss: 0.7314 - acc: 0.7648\n",
      "Epoch 172/200\n",
      " - 204s - loss: 0.7279 - acc: 0.7641\n",
      "Epoch 173/200\n",
      " - 204s - loss: 0.7284 - acc: 0.7645\n",
      "Epoch 174/200\n",
      " - 204s - loss: 0.7260 - acc: 0.7649\n",
      "Epoch 175/200\n",
      " - 206s - loss: 0.7302 - acc: 0.7644\n",
      "Epoch 176/200\n",
      " - 205s - loss: 0.7271 - acc: 0.7620\n",
      "Epoch 177/200\n",
      " - 204s - loss: 0.7245 - acc: 0.7653\n",
      "Epoch 178/200\n",
      " - 204s - loss: 0.7239 - acc: 0.7655\n",
      "Epoch 179/200\n",
      " - 205s - loss: 0.7248 - acc: 0.7652\n",
      "Epoch 180/200\n",
      " - 207s - loss: 0.7193 - acc: 0.7652\n",
      "Epoch 181/200\n",
      " - 205s - loss: 0.7177 - acc: 0.7675\n",
      "Epoch 182/200\n",
      " - 206s - loss: 0.7169 - acc: 0.7660\n",
      "Epoch 183/200\n",
      " - 206s - loss: 0.7172 - acc: 0.7673\n",
      "Epoch 184/200\n",
      " - 206s - loss: 0.7157 - acc: 0.7678\n",
      "Epoch 185/200\n",
      " - 207s - loss: 0.7122 - acc: 0.7697\n",
      "Epoch 186/200\n",
      " - 209s - loss: 0.7135 - acc: 0.7674\n",
      "Epoch 187/200\n",
      " - 206s - loss: 0.7116 - acc: 0.7681\n",
      "Epoch 188/200\n",
      " - 206s - loss: 0.7077 - acc: 0.7698\n",
      "Epoch 189/200\n",
      " - 206s - loss: 0.7082 - acc: 0.7699\n",
      "Epoch 190/200\n",
      " - 206s - loss: 0.7099 - acc: 0.7694\n",
      "Epoch 191/200\n",
      " - 208s - loss: 0.7087 - acc: 0.7690\n",
      "Epoch 192/200\n",
      " - 206s - loss: 0.7003 - acc: 0.7732\n",
      "Epoch 193/200\n",
      " - 206s - loss: 0.7054 - acc: 0.7704\n",
      "Epoch 194/200\n",
      " - 207s - loss: 0.7056 - acc: 0.7703\n",
      "Epoch 195/200\n",
      " - 206s - loss: 0.7053 - acc: 0.7711\n",
      "Epoch 196/200\n",
      " - 208s - loss: 0.7047 - acc: 0.7728\n",
      "Epoch 197/200\n",
      " - 207s - loss: 0.6975 - acc: 0.7737\n",
      "Epoch 198/200\n",
      " - 205s - loss: 0.7000 - acc: 0.7737\n",
      "Epoch 199/200\n",
      " - 204s - loss: 0.7015 - acc: 0.7716\n",
      "Epoch 200/200\n",
      " - 205s - loss: 0.6963 - acc: 0.7731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11733b908>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(LSTM(200, input_shape=(X.shape[1], X.shape[2])))\n",
    "model1.add(Dropout(0.3))\n",
    "model1.add(Dense(200, input_shape=(X.shape[1], X.shape[2])))\n",
    "model1.add(Dropout(0.3))\n",
    "model1.add(Dense(200, input_shape=(X.shape[1], X.shape[2])))\n",
    "model1.add(Dropout(0.3))\n",
    "\n",
    "# adding temperature\n",
    "temp = 1.5\n",
    "model1.add(Lambda(lambda x : x /temp))\n",
    "model1.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "print(model1.summary())\n",
    "# compile model\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "\n",
    "model1.fit(X, y, epochs=200, verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If are another's green and the parted and the parts, \n",
      "To make of mine and still than thou art other, \n",
      "In their beauty of thee thine and so to time, \n",
      "Do his sporttous mine own self to truth of many: \n",
      "My heart which in my dear still and yet so. \n",
      "Should ach that thou hast stall that I  have starn, \n",
      "To most of love that deel'st thou to thee shall: \n",
      "Star for love though not so but decay of all, \n",
      "To the heans and write and master that thou art, \n",
      "I love thee all the world will be it leap: \n",
      "To every to make and my dear still and yet, \n",
      "So is the time that beauty still and you and: \n",
      "Thee in my dode in and my self a bereater, \n",
      "Brand shine and my self alone as my love to? \n",
      "Make me dead that thou shouldst not and the summer, \n",
      "Sweet "
     ]
    }
   ],
   "source": [
    "generate_poem(model1, chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But the time and to my self a moning tor, \n",
      "Shall self alone and thou art the earth says so, \n",
      "Say more reason of your true love to show what: \n",
      "Wealth she can all their state but do not so, \n",
      "I love thee in must barly love and they in. \n",
      "Though not so if thou art forsuned in thy soul. \n",
      "Knows is as a false of heart is thine eye. \n",
      "But thought compare thou art as thom a face and, \n",
      "Swear thou be dost thou to make my self a: \n",
      "Moner of this beauty shall stall in love to stay, \n",
      "I loved o hermakes so things removed that fies ence, \n",
      "As I  love thee all then make the world should: \n",
      "Loved and so I  love thee and all my some: \n",
      "Soon thou dost thou those alone this s in thee: \n",
      "As thou wilt for the sun of this are though, \n",
      "I will o love."
     ]
    }
   ],
   "source": [
    "summers_day_poem(model1, chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperature = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 200)               184000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 29)                5829      \n",
      "=================================================================\n",
      "Total params: 270,229\n",
      "Trainable params: 270,229\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      " - 225s - loss: 2.2222 - acc: 0.3453\n",
      "Epoch 2/200\n",
      " - 215s - loss: 1.9783 - acc: 0.4068\n",
      "Epoch 3/200\n",
      " - 218s - loss: 1.8822 - acc: 0.4351\n",
      "Epoch 4/200\n",
      " - 217s - loss: 1.8169 - acc: 0.4530\n",
      "Epoch 5/200\n",
      " - 219s - loss: 1.7686 - acc: 0.4650\n",
      "Epoch 6/200\n",
      " - 218s - loss: 1.7266 - acc: 0.4786\n",
      "Epoch 7/200\n",
      " - 217s - loss: 1.6900 - acc: 0.4863\n",
      "Epoch 8/200\n",
      " - 218s - loss: 1.6579 - acc: 0.4947\n",
      "Epoch 9/200\n",
      " - 221s - loss: 1.6298 - acc: 0.5028\n",
      "Epoch 10/200\n",
      " - 217s - loss: 1.6032 - acc: 0.5097\n",
      "Epoch 11/200\n",
      " - 216s - loss: 1.5817 - acc: 0.5153\n",
      "Epoch 12/200\n",
      " - 217s - loss: 1.5585 - acc: 0.5216\n",
      "Epoch 13/200\n",
      " - 217s - loss: 1.5379 - acc: 0.5264\n",
      "Epoch 14/200\n",
      " - 218s - loss: 1.5143 - acc: 0.5340\n",
      "Epoch 15/200\n",
      " - 219s - loss: 1.4952 - acc: 0.5366\n",
      "Epoch 16/200\n",
      " - 218s - loss: 1.4769 - acc: 0.5437\n",
      "Epoch 17/200\n",
      " - 218s - loss: 1.4604 - acc: 0.5466\n",
      "Epoch 18/200\n",
      " - 219s - loss: 1.4425 - acc: 0.5512\n",
      "Epoch 19/200\n",
      " - 217s - loss: 1.4253 - acc: 0.5567\n",
      "Epoch 20/200\n",
      " - 216s - loss: 1.4078 - acc: 0.5626\n",
      "Epoch 21/200\n",
      " - 218s - loss: 1.3903 - acc: 0.5678\n",
      "Epoch 22/200\n",
      " - 218s - loss: 1.3760 - acc: 0.5699\n",
      "Epoch 23/200\n",
      " - 216s - loss: 1.3567 - acc: 0.5759\n",
      "Epoch 24/200\n",
      " - 218s - loss: 1.3413 - acc: 0.5797\n",
      "Epoch 25/200\n",
      " - 217s - loss: 1.3300 - acc: 0.5830\n",
      "Epoch 26/200\n",
      " - 219s - loss: 1.3140 - acc: 0.5896\n",
      "Epoch 27/200\n",
      " - 218s - loss: 1.3010 - acc: 0.5922\n",
      "Epoch 28/200\n",
      " - 218s - loss: 1.2824 - acc: 0.5983\n",
      "Epoch 29/200\n",
      " - 218s - loss: 1.2712 - acc: 0.6010\n",
      "Epoch 30/200\n",
      " - 219s - loss: 1.2592 - acc: 0.6055\n",
      "Epoch 31/200\n",
      " - 217s - loss: 1.2496 - acc: 0.6081\n",
      "Epoch 32/200\n",
      " - 221s - loss: 1.2343 - acc: 0.6141\n",
      "Epoch 33/200\n",
      " - 218s - loss: 1.2224 - acc: 0.6150\n",
      "Epoch 34/200\n",
      " - 217s - loss: 1.2151 - acc: 0.6185\n",
      "Epoch 35/200\n",
      " - 219s - loss: 1.2015 - acc: 0.6226\n",
      "Epoch 36/200\n",
      " - 220s - loss: 1.1965 - acc: 0.6226\n",
      "Epoch 37/200\n",
      " - 218s - loss: 1.1824 - acc: 0.6280\n",
      "Epoch 38/200\n",
      " - 218s - loss: 1.1778 - acc: 0.6282\n",
      "Epoch 39/200\n",
      " - 218s - loss: 1.1650 - acc: 0.6340\n",
      "Epoch 40/200\n",
      " - 217s - loss: 1.1610 - acc: 0.6352\n",
      "Epoch 41/200\n",
      " - 220s - loss: 1.1493 - acc: 0.6378\n",
      "Epoch 42/200\n",
      " - 217s - loss: 1.1446 - acc: 0.6380\n",
      "Epoch 43/200\n",
      " - 219s - loss: 1.1312 - acc: 0.6430\n",
      "Epoch 44/200\n",
      " - 218s - loss: 1.1250 - acc: 0.6446\n",
      "Epoch 45/200\n",
      " - 219s - loss: 1.1152 - acc: 0.6473\n",
      "Epoch 46/200\n",
      " - 216s - loss: 1.1121 - acc: 0.6488\n",
      "Epoch 47/200\n",
      " - 218s - loss: 1.1028 - acc: 0.6507\n",
      "Epoch 48/200\n",
      " - 219s - loss: 1.0963 - acc: 0.6523\n",
      "Epoch 49/200\n",
      " - 219s - loss: 1.0906 - acc: 0.6538\n",
      "Epoch 50/200\n",
      " - 219s - loss: 1.0839 - acc: 0.6555\n",
      "Epoch 51/200\n",
      " - 218s - loss: 1.0792 - acc: 0.6572\n",
      "Epoch 52/200\n",
      " - 218s - loss: 1.0707 - acc: 0.6611\n",
      "Epoch 53/200\n",
      " - 219s - loss: 1.0643 - acc: 0.6627\n",
      "Epoch 54/200\n",
      " - 220s - loss: 1.0574 - acc: 0.6643\n",
      "Epoch 55/200\n",
      " - 219s - loss: 1.0523 - acc: 0.6667\n",
      "Epoch 56/200\n",
      " - 218s - loss: 1.0517 - acc: 0.6649\n",
      "Epoch 57/200\n",
      " - 219s - loss: 1.0362 - acc: 0.6718\n",
      "Epoch 58/200\n",
      " - 219s - loss: 1.0347 - acc: 0.6723\n",
      "Epoch 59/200\n",
      " - 218s - loss: 1.0329 - acc: 0.6724\n",
      "Epoch 60/200\n",
      " - 218s - loss: 1.0241 - acc: 0.6742\n",
      "Epoch 61/200\n",
      " - 219s - loss: 1.0181 - acc: 0.6778\n",
      "Epoch 62/200\n",
      " - 218s - loss: 1.0130 - acc: 0.6773\n",
      "Epoch 63/200\n",
      " - 221s - loss: 1.0133 - acc: 0.6780\n",
      "Epoch 64/200\n",
      " - 218s - loss: 1.0065 - acc: 0.6798\n",
      "Epoch 65/200\n",
      " - 219s - loss: 1.0004 - acc: 0.6826\n",
      "Epoch 66/200\n",
      " - 218s - loss: 0.9982 - acc: 0.6829\n",
      "Epoch 67/200\n",
      " - 218s - loss: 0.9899 - acc: 0.6860\n",
      "Epoch 68/200\n",
      " - 220s - loss: 0.9861 - acc: 0.6854\n",
      "Epoch 69/200\n",
      " - 219s - loss: 0.9813 - acc: 0.6903\n",
      "Epoch 70/200\n",
      " - 217s - loss: 0.9773 - acc: 0.6889\n",
      "Epoch 71/200\n",
      " - 218s - loss: 0.9759 - acc: 0.6877\n",
      "Epoch 72/200\n",
      " - 217s - loss: 0.9711 - acc: 0.6917\n",
      "Epoch 73/200\n",
      " - 221s - loss: 0.9675 - acc: 0.6911\n",
      "Epoch 74/200\n",
      " - 218s - loss: 0.9672 - acc: 0.6933\n",
      "Epoch 75/200\n",
      " - 218s - loss: 0.9572 - acc: 0.6962\n",
      "Epoch 76/200\n",
      " - 218s - loss: 0.9541 - acc: 0.6960\n",
      "Epoch 77/200\n",
      " - 218s - loss: 0.9543 - acc: 0.6958\n",
      "Epoch 78/200\n",
      " - 220s - loss: 0.9498 - acc: 0.6968\n",
      "Epoch 79/200\n",
      " - 218s - loss: 0.9435 - acc: 0.6999\n",
      "Epoch 80/200\n",
      " - 217s - loss: 0.9415 - acc: 0.7007\n",
      "Epoch 81/200\n",
      " - 219s - loss: 0.9397 - acc: 0.7008\n",
      "Epoch 82/200\n",
      " - 220s - loss: 0.9384 - acc: 0.7007\n",
      "Epoch 83/200\n",
      " - 219s - loss: 0.9338 - acc: 0.7021\n",
      "Epoch 84/200\n",
      " - 219s - loss: 0.9261 - acc: 0.7043\n",
      "Epoch 85/200\n",
      " - 219s - loss: 0.9247 - acc: 0.7059\n",
      "Epoch 86/200\n",
      " - 218s - loss: 0.9224 - acc: 0.7066\n",
      "Epoch 87/200\n",
      " - 218s - loss: 0.9211 - acc: 0.7059\n",
      "Epoch 88/200\n",
      " - 217s - loss: 0.9126 - acc: 0.7096\n",
      "Epoch 89/200\n",
      " - 218s - loss: 0.9120 - acc: 0.7097\n",
      "Epoch 90/200\n",
      " - 219s - loss: 0.9076 - acc: 0.7122\n",
      "Epoch 91/200\n",
      " - 220s - loss: 0.9045 - acc: 0.7111\n",
      "Epoch 92/200\n",
      " - 218s - loss: 0.9005 - acc: 0.7120\n",
      "Epoch 93/200\n",
      " - 218s - loss: 0.8970 - acc: 0.7129\n",
      "Epoch 94/200\n",
      " - 218s - loss: 0.8983 - acc: 0.7123\n",
      "Epoch 95/200\n",
      " - 218s - loss: 0.8942 - acc: 0.7146\n",
      "Epoch 96/200\n",
      " - 218s - loss: 0.8890 - acc: 0.7167\n",
      "Epoch 97/200\n",
      " - 219s - loss: 0.8904 - acc: 0.7141\n",
      "Epoch 98/200\n",
      " - 218s - loss: 0.8853 - acc: 0.7172\n",
      "Epoch 99/200\n",
      " - 219s - loss: 0.8826 - acc: 0.7180\n",
      "Epoch 100/200\n",
      " - 217s - loss: 0.8808 - acc: 0.7182\n",
      "Epoch 101/200\n",
      " - 219s - loss: 0.8799 - acc: 0.7189\n",
      "Epoch 102/200\n",
      " - 219s - loss: 0.8760 - acc: 0.7201\n",
      "Epoch 103/200\n",
      " - 218s - loss: 0.8797 - acc: 0.7181\n",
      "Epoch 104/200\n",
      " - 219s - loss: 0.8727 - acc: 0.7215\n",
      "Epoch 105/200\n",
      " - 219s - loss: 0.8667 - acc: 0.7229\n",
      "Epoch 106/200\n",
      " - 218s - loss: 0.8652 - acc: 0.7229\n",
      "Epoch 107/200\n",
      " - 219s - loss: 0.8643 - acc: 0.7245\n",
      "Epoch 108/200\n",
      " - 221s - loss: 0.8622 - acc: 0.7250\n",
      "Epoch 109/200\n",
      " - 219s - loss: 0.8578 - acc: 0.7249\n",
      "Epoch 110/200\n",
      " - 219s - loss: 0.8551 - acc: 0.7264\n",
      "Epoch 111/200\n",
      " - 218s - loss: 0.8502 - acc: 0.7274\n",
      "Epoch 112/200\n",
      " - 221s - loss: 0.8527 - acc: 0.7265\n",
      "Epoch 113/200\n",
      " - 219s - loss: 0.8507 - acc: 0.7290\n",
      "Epoch 114/200\n",
      " - 219s - loss: 0.8530 - acc: 0.7253\n",
      "Epoch 115/200\n",
      " - 217s - loss: 0.8461 - acc: 0.7284\n",
      "Epoch 116/200\n",
      " - 218s - loss: 0.8436 - acc: 0.7299\n",
      "Epoch 117/200\n",
      " - 218s - loss: 0.8407 - acc: 0.7294\n",
      "Epoch 118/200\n",
      " - 218s - loss: 0.8321 - acc: 0.7328\n",
      "Epoch 119/200\n",
      " - 217s - loss: 0.8435 - acc: 0.7292\n",
      "Epoch 120/200\n",
      " - 219s - loss: 0.8370 - acc: 0.7328\n",
      "Epoch 121/200\n",
      " - 219s - loss: 0.8357 - acc: 0.7323\n",
      "Epoch 122/200\n",
      " - 219s - loss: 0.8259 - acc: 0.7350\n",
      "Epoch 123/200\n",
      " - 219s - loss: 0.8319 - acc: 0.7316\n",
      "Epoch 124/200\n",
      " - 218s - loss: 0.8247 - acc: 0.7341\n",
      "Epoch 125/200\n",
      " - 217s - loss: 0.8253 - acc: 0.7368\n",
      "Epoch 126/200\n",
      " - 218s - loss: 0.8226 - acc: 0.7372\n",
      "Epoch 127/200\n",
      " - 216s - loss: 0.8219 - acc: 0.7366\n",
      "Epoch 128/200\n",
      " - 218s - loss: 0.8191 - acc: 0.7366\n",
      "Epoch 129/200\n",
      " - 220s - loss: 0.8180 - acc: 0.7384\n",
      "Epoch 130/200\n",
      " - 219s - loss: 0.8207 - acc: 0.7373\n",
      "Epoch 131/200\n",
      " - 216s - loss: 0.8104 - acc: 0.7403\n",
      "Epoch 132/200\n",
      " - 219s - loss: 0.8106 - acc: 0.7407\n",
      "Epoch 133/200\n",
      " - 219s - loss: 0.8145 - acc: 0.7399\n",
      "Epoch 134/200\n",
      " - 218s - loss: 0.8080 - acc: 0.7406\n",
      "Epoch 135/200\n",
      " - 218s - loss: 0.8064 - acc: 0.7410\n",
      "Epoch 136/200\n",
      " - 218s - loss: 0.8063 - acc: 0.7422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/200\n",
      " - 219s - loss: 0.8044 - acc: 0.7416\n",
      "Epoch 138/200\n",
      " - 218s - loss: 0.8047 - acc: 0.7424\n",
      "Epoch 139/200\n",
      " - 219s - loss: 0.7954 - acc: 0.7459\n",
      "Epoch 140/200\n",
      " - 217s - loss: 0.7973 - acc: 0.7454\n",
      "Epoch 141/200\n",
      " - 220s - loss: 0.7917 - acc: 0.7454\n",
      "Epoch 142/200\n",
      " - 223s - loss: 0.7952 - acc: 0.7441\n",
      "Epoch 143/200\n",
      " - 219s - loss: 0.7869 - acc: 0.7479\n",
      "Epoch 144/200\n",
      " - 219s - loss: 0.7933 - acc: 0.7446\n",
      "Epoch 145/200\n",
      " - 219s - loss: 0.7939 - acc: 0.7451\n",
      "Epoch 146/200\n",
      " - 219s - loss: 0.7853 - acc: 0.7495\n",
      "Epoch 147/200\n",
      " - 221s - loss: 0.7881 - acc: 0.7478\n",
      "Epoch 148/200\n",
      " - 219s - loss: 0.7805 - acc: 0.7493\n",
      "Epoch 149/200\n",
      " - 218s - loss: 0.7845 - acc: 0.7480\n",
      "Epoch 150/200\n",
      " - 219s - loss: 0.7841 - acc: 0.7474\n",
      "Epoch 151/200\n",
      " - 218s - loss: 0.7793 - acc: 0.7503\n",
      "Epoch 152/200\n",
      " - 221s - loss: 0.7790 - acc: 0.7501\n",
      "Epoch 153/200\n",
      " - 217s - loss: 0.7768 - acc: 0.7496\n",
      "Epoch 154/200\n",
      " - 218s - loss: 0.7722 - acc: 0.7522\n",
      "Epoch 155/200\n",
      " - 219s - loss: 0.7756 - acc: 0.7512\n",
      "Epoch 156/200\n",
      " - 218s - loss: 0.7719 - acc: 0.7517\n",
      "Epoch 157/200\n",
      " - 219s - loss: 0.7725 - acc: 0.7516\n",
      "Epoch 158/200\n",
      " - 218s - loss: 0.7650 - acc: 0.7533\n",
      "Epoch 159/200\n",
      " - 216s - loss: 0.7670 - acc: 0.7542\n",
      "Epoch 160/200\n",
      " - 218s - loss: 0.7676 - acc: 0.7540\n",
      "Epoch 161/200\n",
      " - 217s - loss: 0.7701 - acc: 0.7527\n",
      "Epoch 162/200\n",
      " - 220s - loss: 0.7628 - acc: 0.7541\n",
      "Epoch 163/200\n",
      " - 219s - loss: 0.7665 - acc: 0.7542\n",
      "Epoch 164/200\n",
      " - 217s - loss: 0.7585 - acc: 0.7547\n",
      "Epoch 165/200\n",
      " - 218s - loss: 0.7612 - acc: 0.7553\n",
      "Epoch 166/200\n",
      " - 218s - loss: 0.7586 - acc: 0.7567\n",
      "Epoch 167/200\n",
      " - 220s - loss: 0.7564 - acc: 0.7567\n",
      "Epoch 168/200\n",
      " - 217s - loss: 0.7557 - acc: 0.7569\n",
      "Epoch 169/200\n",
      " - 218s - loss: 0.7557 - acc: 0.7567\n",
      "Epoch 170/200\n",
      " - 218s - loss: 0.7511 - acc: 0.7568\n",
      "Epoch 171/200\n",
      " - 217s - loss: 0.7522 - acc: 0.7585\n",
      "Epoch 172/200\n",
      " - 219s - loss: 0.7501 - acc: 0.7590\n",
      "Epoch 173/200\n",
      " - 217s - loss: 0.7502 - acc: 0.7586\n",
      "Epoch 174/200\n",
      " - 219s - loss: 0.7469 - acc: 0.7598\n",
      "Epoch 175/200\n",
      " - 218s - loss: 0.7475 - acc: 0.7591\n",
      "Epoch 176/200\n",
      " - 219s - loss: 0.7477 - acc: 0.7600\n",
      "Epoch 177/200\n",
      " - 220s - loss: 0.7448 - acc: 0.7601\n",
      "Epoch 178/200\n",
      " - 217s - loss: 0.7498 - acc: 0.7594\n",
      "Epoch 179/200\n",
      " - 218s - loss: 0.7401 - acc: 0.7614\n",
      "Epoch 180/200\n",
      " - 219s - loss: 0.7369 - acc: 0.7619\n",
      "Epoch 181/200\n",
      " - 218s - loss: 0.7389 - acc: 0.7615\n",
      "Epoch 182/200\n",
      " - 220s - loss: 0.7407 - acc: 0.7625\n",
      "Epoch 183/200\n",
      " - 219s - loss: 0.7397 - acc: 0.7616\n",
      "Epoch 184/200\n",
      " - 218s - loss: 0.7368 - acc: 0.7628\n",
      "Epoch 185/200\n",
      " - 218s - loss: 0.7361 - acc: 0.7635\n",
      "Epoch 186/200\n",
      " - 221s - loss: 0.7323 - acc: 0.7651\n",
      "Epoch 187/200\n",
      " - 218s - loss: 0.7319 - acc: 0.7646\n",
      "Epoch 188/200\n",
      " - 219s - loss: 0.7307 - acc: 0.7651\n",
      "Epoch 189/200\n",
      " - 219s - loss: 0.7309 - acc: 0.7653\n",
      "Epoch 190/200\n",
      " - 219s - loss: 0.7348 - acc: 0.7623\n",
      "Epoch 191/200\n",
      " - 219s - loss: 0.7252 - acc: 0.7656\n",
      "Epoch 192/200\n",
      " - 219s - loss: 0.7262 - acc: 0.7661\n",
      "Epoch 193/200\n",
      " - 222s - loss: 0.7251 - acc: 0.7657\n",
      "Epoch 194/200\n",
      " - 219s - loss: 0.7271 - acc: 0.7655\n",
      "Epoch 195/200\n",
      " - 220s - loss: 0.7229 - acc: 0.7654\n",
      "Epoch 196/200\n",
      " - 221s - loss: 0.7221 - acc: 0.7665\n",
      "Epoch 197/200\n",
      " - 218s - loss: 0.7214 - acc: 0.7667\n",
      "Epoch 198/200\n",
      " - 218s - loss: 0.7233 - acc: 0.7672\n",
      "Epoch 199/200\n",
      " - 218s - loss: 0.7257 - acc: 0.7658\n",
      "Epoch 200/200\n",
      " - 219s - loss: 0.7208 - acc: 0.7684\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x116751a58>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(LSTM(200, input_shape=(X.shape[1], X.shape[2])))\n",
    "model2.add(Dropout(0.3))\n",
    "model2.add(Dense(200, input_shape=(X.shape[1], X.shape[2])))\n",
    "model2.add(Dropout(0.3))\n",
    "model2.add(Dense(200, input_shape=(X.shape[1], X.shape[2])))\n",
    "model2.add(Dropout(0.3))\n",
    "\n",
    "# adding temperature\n",
    "temp = 0.75\n",
    "model2.add(Lambda(lambda x : x /temp))\n",
    "model2.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "print(model2.summary())\n",
    "# compile model\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "\n",
    "model2.fit(X, y, epochs=200, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thou art all the world and summer's floend which should. \n",
      "That which it be astell on my love being my, \n",
      "Part and therefore not so I  love thee in the, \n",
      "Part and will be but you may in my mind, \n",
      "That from the call of thee this spring which nightly: \n",
      "Graces that writ in one thing and therefore to be. \n",
      "Thence and straight and or cold and true and lives, \n",
      "Thoughts and beauty lardenhig ward and beauty laid in heaven, \n",
      "Is so stang as thou shalt be old which her: \n",
      "Before shall beauteous seem in the wide with teems that, \n",
      "Find the least that I  am not love that love: \n",
      "Is beauty not before than gavelest seeming that thou art. \n",
      "Therefore to be as a stars and worth to see: \n",
      "To the stars and something breast and sometime and."
     ]
    }
   ],
   "source": [
    "generate_poem(model2, chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Art thou art as thoughts and theast thou beauteous nigmanarale, \n",
      "May no make in then make more let me so, \n",
      "Not thou mayst to be as plack wherite hold in, \n",
      "Them in the mind of me to be so shall: \n",
      "I spend in memory the mine eye say to the? \n",
      "Terming strange that in the eyes ware still the praise, \n",
      "And sometime that first is black and lovest fairent whine, \n",
      "I am statious not before thences and love to make? \n",
      "Me love is are the mained face shall be but. \n",
      "From the say their hers very fine or heaven thou: \n",
      "Art and say it to my verse to his shade, \n",
      "When it is all alone to each to know my, \n",
      "Friend's hust before to be as as as the truant. \n",
      "In the pearly drat the baak straight in seeming the, \n",
      "Self are not my live them for my s."
     ]
    }
   ],
   "source": [
    "summers_day_poem(model2, chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperature = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 200)               184000    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 29)                5829      \n",
      "=================================================================\n",
      "Total params: 270,229\n",
      "Trainable params: 270,229\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      " - 250s - loss: 2.2356 - acc: 0.3425\n",
      "Epoch 2/200\n",
      " - 247s - loss: 2.0127 - acc: 0.3983\n",
      "Epoch 3/200\n",
      " - 242s - loss: 1.9176 - acc: 0.4261\n",
      "Epoch 4/200\n",
      " - 254s - loss: 1.8486 - acc: 0.4443\n",
      "Epoch 5/200\n",
      " - 232s - loss: 1.8005 - acc: 0.4580\n",
      "Epoch 6/200\n",
      " - 223s - loss: 1.7566 - acc: 0.4673\n",
      "Epoch 7/200\n",
      " - 229s - loss: 1.7216 - acc: 0.4798\n",
      "Epoch 8/200\n",
      " - 236s - loss: 1.6903 - acc: 0.4879\n",
      "Epoch 9/200\n",
      " - 250s - loss: 1.6605 - acc: 0.4951\n",
      "Epoch 10/200\n",
      " - 248s - loss: 1.6420 - acc: 0.4979\n",
      "Epoch 11/200\n",
      " - 243s - loss: 1.6144 - acc: 0.5060\n",
      "Epoch 12/200\n",
      " - 229s - loss: 1.5935 - acc: 0.5127\n",
      "Epoch 13/200\n",
      " - 234s - loss: 1.5748 - acc: 0.5163\n",
      "Epoch 14/200\n",
      " - 232s - loss: 1.5548 - acc: 0.5214\n",
      "Epoch 15/200\n",
      " - 238s - loss: 1.5376 - acc: 0.5273\n",
      "Epoch 16/200\n",
      " - 227s - loss: 1.5204 - acc: 0.5317\n",
      "Epoch 17/200\n",
      " - 253s - loss: 1.5047 - acc: 0.5355\n",
      "Epoch 18/200\n",
      " - 256s - loss: 1.4870 - acc: 0.5399\n",
      "Epoch 19/200\n",
      " - 226s - loss: 1.4713 - acc: 0.5456\n",
      "Epoch 20/200\n",
      " - 242s - loss: 1.4549 - acc: 0.5495\n",
      "Epoch 21/200\n",
      " - 244s - loss: 1.4406 - acc: 0.5529\n",
      "Epoch 22/200\n",
      " - 234s - loss: 1.4283 - acc: 0.5555\n",
      "Epoch 23/200\n",
      " - 220s - loss: 1.4125 - acc: 0.5604\n",
      "Epoch 24/200\n",
      " - 232s - loss: 1.3970 - acc: 0.5668\n",
      "Epoch 25/200\n",
      " - 250s - loss: 1.3820 - acc: 0.5689\n",
      "Epoch 26/200\n",
      " - 221s - loss: 1.3704 - acc: 0.5721\n",
      "Epoch 27/200\n",
      " - 238s - loss: 1.3534 - acc: 0.5781\n",
      "Epoch 28/200\n",
      " - 239s - loss: 1.3442 - acc: 0.5812\n",
      "Epoch 29/200\n",
      " - 240s - loss: 1.3318 - acc: 0.5843\n",
      "Epoch 30/200\n",
      " - 244s - loss: 1.3217 - acc: 0.5863\n",
      "Epoch 31/200\n",
      " - 243s - loss: 1.3082 - acc: 0.5918\n",
      "Epoch 32/200\n",
      " - 220s - loss: 1.2939 - acc: 0.5947\n",
      "Epoch 33/200\n",
      " - 209s - loss: 1.2844 - acc: 0.5965\n",
      "Epoch 34/200\n",
      " - 208s - loss: 1.2771 - acc: 0.5988\n",
      "Epoch 35/200\n",
      " - 209s - loss: 1.2620 - acc: 0.6042\n",
      "Epoch 36/200\n",
      " - 209s - loss: 1.2523 - acc: 0.6082\n",
      "Epoch 37/200\n",
      " - 214s - loss: 1.2440 - acc: 0.6090\n",
      "Epoch 38/200\n",
      " - 220s - loss: 1.2346 - acc: 0.6113\n",
      "Epoch 39/200\n",
      " - 219s - loss: 1.2232 - acc: 0.6161\n",
      "Epoch 40/200\n",
      " - 222s - loss: 1.2162 - acc: 0.6175\n",
      "Epoch 41/200\n",
      " - 219s - loss: 1.2084 - acc: 0.6197\n",
      "Epoch 42/200\n",
      " - 220s - loss: 1.2006 - acc: 0.6231\n",
      "Epoch 43/200\n",
      " - 219s - loss: 1.1942 - acc: 0.6243\n",
      "Epoch 44/200\n",
      " - 220s - loss: 1.1851 - acc: 0.6296\n",
      "Epoch 45/200\n",
      " - 221s - loss: 1.1768 - acc: 0.6294\n",
      "Epoch 46/200\n",
      " - 220s - loss: 1.1642 - acc: 0.6338\n",
      "Epoch 47/200\n",
      " - 220s - loss: 1.1626 - acc: 0.6334\n",
      "Epoch 48/200\n",
      " - 220s - loss: 1.1508 - acc: 0.6374\n",
      "Epoch 49/200\n",
      " - 219s - loss: 1.1445 - acc: 0.6394\n",
      "Epoch 50/200\n",
      " - 222s - loss: 1.1387 - acc: 0.6393\n",
      "Epoch 51/200\n",
      " - 219s - loss: 1.1307 - acc: 0.6444\n",
      "Epoch 52/200\n",
      " - 218s - loss: 1.1206 - acc: 0.6466\n",
      "Epoch 53/200\n",
      " - 218s - loss: 1.1188 - acc: 0.6480\n",
      "Epoch 54/200\n",
      " - 220s - loss: 1.1131 - acc: 0.6497\n",
      "Epoch 55/200\n",
      " - 218s - loss: 1.1088 - acc: 0.6503\n",
      "Epoch 56/200\n",
      " - 219s - loss: 1.0984 - acc: 0.6529\n",
      "Epoch 57/200\n",
      " - 221s - loss: 1.0948 - acc: 0.6548\n",
      "Epoch 58/200\n",
      " - 219s - loss: 1.0928 - acc: 0.6552\n",
      "Epoch 59/200\n",
      " - 220s - loss: 1.0801 - acc: 0.6588\n",
      "Epoch 60/200\n",
      " - 221s - loss: 1.0795 - acc: 0.6591\n",
      "Epoch 61/200\n",
      " - 220s - loss: 1.0669 - acc: 0.6628\n",
      "Epoch 62/200\n",
      " - 220s - loss: 1.0654 - acc: 0.6635\n",
      "Epoch 63/200\n",
      " - 221s - loss: 1.0601 - acc: 0.6652\n",
      "Epoch 64/200\n",
      " - 219s - loss: 1.0580 - acc: 0.6650\n",
      "Epoch 65/200\n",
      " - 219s - loss: 1.0480 - acc: 0.6664\n",
      "Epoch 66/200\n",
      " - 219s - loss: 1.0432 - acc: 0.6713\n",
      "Epoch 67/200\n",
      " - 220s - loss: 1.0376 - acc: 0.6718\n",
      "Epoch 68/200\n",
      " - 220s - loss: 1.0313 - acc: 0.6749\n",
      "Epoch 69/200\n",
      " - 220s - loss: 1.0361 - acc: 0.6726\n",
      "Epoch 70/200\n",
      " - 218s - loss: 1.0303 - acc: 0.6740\n",
      "Epoch 71/200\n",
      " - 218s - loss: 1.0205 - acc: 0.6767\n",
      "Epoch 72/200\n",
      " - 220s - loss: 1.0200 - acc: 0.6773\n",
      "Epoch 73/200\n",
      " - 219s - loss: 1.0149 - acc: 0.6793\n",
      "Epoch 74/200\n",
      " - 218s - loss: 1.0101 - acc: 0.6796\n",
      "Epoch 75/200\n",
      " - 223s - loss: 1.0109 - acc: 0.6809\n",
      "Epoch 76/200\n",
      " - 217s - loss: 1.0046 - acc: 0.6810\n",
      "Epoch 77/200\n",
      " - 215s - loss: 0.9990 - acc: 0.6833\n",
      "Epoch 78/200\n",
      " - 214s - loss: 0.9974 - acc: 0.6835\n",
      "Epoch 79/200\n",
      " - 219s - loss: 0.9930 - acc: 0.6851\n",
      "Epoch 80/200\n",
      " - 233s - loss: 0.9915 - acc: 0.6853\n",
      "Epoch 81/200\n",
      " - 213s - loss: 0.9839 - acc: 0.6882\n",
      "Epoch 82/200\n",
      " - 220s - loss: 0.9843 - acc: 0.6880\n",
      "Epoch 83/200\n",
      " - 220s - loss: 0.9839 - acc: 0.6881\n",
      "Epoch 84/200\n",
      " - 240s - loss: 0.9747 - acc: 0.6907\n",
      "Epoch 85/200\n",
      " - 250s - loss: 1.0413 - acc: 0.6756\n",
      "Epoch 86/200\n",
      " - 215s - loss: 1.0255 - acc: 0.6778\n",
      "Epoch 87/200\n",
      " - 218s - loss: 0.9802 - acc: 0.6901\n",
      "Epoch 88/200\n",
      " - 221s - loss: 0.9871 - acc: 0.6885\n",
      "Epoch 89/200\n",
      " - 224s - loss: 0.9791 - acc: 0.6899\n",
      "Epoch 90/200\n",
      " - 208s - loss: 0.9904 - acc: 0.6884\n",
      "Epoch 91/200\n",
      " - 209s - loss: 0.9592 - acc: 0.6957\n",
      "Epoch 92/200\n",
      " - 219s - loss: 0.9664 - acc: 0.6936\n",
      "Epoch 93/200\n",
      " - 217s - loss: 0.9700 - acc: 0.6926\n",
      "Epoch 94/200\n",
      " - 220s - loss: 0.9570 - acc: 0.6971\n",
      "Epoch 95/200\n",
      " - 220s - loss: 0.9570 - acc: 0.6979\n",
      "Epoch 96/200\n",
      " - 219s - loss: 0.9493 - acc: 0.6999\n",
      "Epoch 97/200\n",
      " - 223s - loss: 0.9489 - acc: 0.7000\n",
      "Epoch 98/200\n",
      " - 225s - loss: 0.9426 - acc: 0.7012\n",
      "Epoch 99/200\n",
      " - 211s - loss: 0.9415 - acc: 0.7023\n",
      "Epoch 100/200\n",
      " - 219s - loss: 0.9447 - acc: 0.7002\n",
      "Epoch 101/200\n",
      " - 240s - loss: 0.9341 - acc: 0.7042\n",
      "Epoch 102/200\n",
      " - 218s - loss: 0.9333 - acc: 0.7026\n",
      "Epoch 103/200\n",
      " - 212s - loss: 0.9322 - acc: 0.7040\n",
      "Epoch 104/200\n",
      " - 209s - loss: 0.9124 - acc: 0.7097\n",
      "Epoch 105/200\n",
      " - 225s - loss: 0.9224 - acc: 0.7066\n",
      "Epoch 106/200\n",
      " - 220s - loss: 0.9225 - acc: 0.7065\n",
      "Epoch 107/200\n",
      " - 219s - loss: 0.9209 - acc: 0.7077\n",
      "Epoch 108/200\n",
      " - 222s - loss: 0.9126 - acc: 0.7108\n",
      "Epoch 109/200\n",
      " - 221s - loss: 0.9106 - acc: 0.7105\n",
      "Epoch 110/200\n",
      " - 221s - loss: 0.9096 - acc: 0.7105\n",
      "Epoch 111/200\n",
      " - 230s - loss: 0.9058 - acc: 0.7123\n",
      "Epoch 112/200\n",
      " - 238s - loss: 0.9059 - acc: 0.7113\n",
      "Epoch 113/200\n",
      " - 224s - loss: 0.9002 - acc: 0.7131\n",
      "Epoch 114/200\n",
      " - 223s - loss: 0.8958 - acc: 0.7143\n",
      "Epoch 115/200\n",
      " - 218s - loss: 0.8937 - acc: 0.7162\n",
      "Epoch 116/200\n",
      " - 219s - loss: 0.8891 - acc: 0.7172\n",
      "Epoch 117/200\n",
      " - 218s - loss: 0.8921 - acc: 0.7158\n",
      "Epoch 118/200\n",
      " - 222s - loss: 0.8892 - acc: 0.7183\n",
      "Epoch 119/200\n",
      " - 219s - loss: 0.8860 - acc: 0.7182\n",
      "Epoch 120/200\n",
      " - 219s - loss: 0.8807 - acc: 0.7189\n",
      "Epoch 121/200\n",
      " - 220s - loss: 0.8770 - acc: 0.7215\n",
      "Epoch 122/200\n",
      " - 221s - loss: 0.8800 - acc: 0.7203\n",
      "Epoch 123/200\n",
      " - 219s - loss: 0.8764 - acc: 0.7207\n",
      "Epoch 124/200\n",
      " - 220s - loss: 0.8757 - acc: 0.7208\n",
      "Epoch 125/200\n",
      " - 222s - loss: 0.8725 - acc: 0.7233\n",
      "Epoch 126/200\n",
      " - 221s - loss: 0.8668 - acc: 0.7252\n",
      "Epoch 127/200\n",
      " - 232s - loss: 0.8658 - acc: 0.7248\n",
      "Epoch 128/200\n",
      " - 210s - loss: 0.8680 - acc: 0.7247\n",
      "Epoch 129/200\n",
      " - 209s - loss: 0.8657 - acc: 0.7239\n",
      "Epoch 130/200\n",
      " - 242s - loss: 0.8566 - acc: 0.7257\n",
      "Epoch 131/200\n",
      " - 229s - loss: 0.8628 - acc: 0.7258\n",
      "Epoch 132/200\n",
      " - 229s - loss: 0.8569 - acc: 0.7282\n",
      "Epoch 133/200\n",
      " - 230s - loss: 0.8571 - acc: 0.7262\n",
      "Epoch 134/200\n",
      " - 230s - loss: 0.8555 - acc: 0.7285\n",
      "Epoch 135/200\n",
      " - 215s - loss: 0.8478 - acc: 0.7295\n",
      "Epoch 136/200\n",
      " - 210s - loss: 0.8504 - acc: 0.7288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/200\n",
      " - 209s - loss: 0.8456 - acc: 0.7305\n",
      "Epoch 138/200\n",
      " - 210s - loss: 0.8421 - acc: 0.7315\n",
      "Epoch 139/200\n",
      " - 213s - loss: 0.8425 - acc: 0.7323\n",
      "Epoch 140/200\n",
      " - 221s - loss: 0.8420 - acc: 0.7320\n",
      "Epoch 141/200\n",
      " - 221s - loss: 0.8392 - acc: 0.7312\n",
      "Epoch 142/200\n",
      " - 221s - loss: 0.8397 - acc: 0.7336\n",
      "Epoch 143/200\n",
      " - 220s - loss: 0.8309 - acc: 0.7343\n",
      "Epoch 144/200\n",
      " - 219s - loss: 0.8328 - acc: 0.7357\n",
      "Epoch 145/200\n",
      " - 220s - loss: 0.8312 - acc: 0.7341\n",
      "Epoch 146/200\n",
      " - 236s - loss: 0.8269 - acc: 0.7361\n",
      "Epoch 147/200\n",
      " - 220s - loss: 0.8291 - acc: 0.7367\n",
      "Epoch 148/200\n",
      " - 216s - loss: 0.8235 - acc: 0.7367\n",
      "Epoch 149/200\n",
      " - 218s - loss: 0.8276 - acc: 0.7365\n",
      "Epoch 150/200\n",
      " - 216s - loss: 0.8243 - acc: 0.7379\n",
      "Epoch 151/200\n",
      " - 239s - loss: 0.8218 - acc: 0.7364\n",
      "Epoch 152/200\n",
      " - 244s - loss: 0.8239 - acc: 0.7379\n",
      "Epoch 153/200\n",
      " - 216s - loss: 0.8184 - acc: 0.7388\n",
      "Epoch 154/200\n",
      " - 215s - loss: 0.8149 - acc: 0.7398\n",
      "Epoch 155/200\n",
      " - 238s - loss: 0.8148 - acc: 0.7409\n",
      "Epoch 156/200\n",
      " - 220s - loss: 0.8117 - acc: 0.7418\n",
      "Epoch 157/200\n",
      " - 220s - loss: 0.8083 - acc: 0.7414\n",
      "Epoch 158/200\n",
      " - 229s - loss: 0.8116 - acc: 0.7399\n",
      "Epoch 159/200\n",
      " - 218s - loss: 0.8052 - acc: 0.7431\n",
      "Epoch 160/200\n",
      " - 218s - loss: 0.8051 - acc: 0.7440\n",
      "Epoch 161/200\n",
      " - 217s - loss: 0.8032 - acc: 0.7437\n",
      "Epoch 162/200\n",
      " - 218s - loss: 0.8039 - acc: 0.7443\n",
      "Epoch 163/200\n",
      " - 218s - loss: 0.7996 - acc: 0.7443\n",
      "Epoch 164/200\n",
      " - 217s - loss: 0.8003 - acc: 0.7447\n",
      "Epoch 165/200\n",
      " - 220s - loss: 0.7930 - acc: 0.7473\n",
      "Epoch 166/200\n",
      " - 218s - loss: 0.7951 - acc: 0.7470\n",
      "Epoch 167/200\n",
      " - 217s - loss: 0.7965 - acc: 0.7458\n",
      "Epoch 168/200\n",
      " - 217s - loss: 0.7919 - acc: 0.7468\n",
      "Epoch 169/200\n",
      " - 219s - loss: 0.7936 - acc: 0.7476\n",
      "Epoch 170/200\n",
      " - 218s - loss: 0.7888 - acc: 0.7483\n",
      "Epoch 171/200\n",
      " - 219s - loss: 0.7896 - acc: 0.7474\n",
      "Epoch 172/200\n",
      " - 210s - loss: 0.7891 - acc: 0.7483\n",
      "Epoch 173/200\n",
      " - 210s - loss: 0.7842 - acc: 0.7509\n",
      "Epoch 174/200\n",
      " - 213s - loss: 0.7842 - acc: 0.7503\n",
      "Epoch 175/200\n",
      " - 208s - loss: 0.7789 - acc: 0.7509\n",
      "Epoch 176/200\n",
      " - 209s - loss: 0.7857 - acc: 0.7503\n",
      "Epoch 177/200\n",
      " - 211s - loss: 0.7821 - acc: 0.7494\n",
      "Epoch 178/200\n",
      " - 224s - loss: 0.7833 - acc: 0.7487\n",
      "Epoch 179/200\n",
      " - 222s - loss: 0.7807 - acc: 0.7512\n",
      "Epoch 180/200\n",
      " - 220s - loss: 0.7790 - acc: 0.7512\n",
      "Epoch 181/200\n",
      " - 220s - loss: 0.7751 - acc: 0.7520\n",
      "Epoch 182/200\n",
      " - 218s - loss: 0.7738 - acc: 0.7526\n",
      "Epoch 183/200\n",
      " - 228s - loss: 0.7763 - acc: 0.7525\n",
      "Epoch 184/200\n",
      " - 222s - loss: 0.7725 - acc: 0.7522\n",
      "Epoch 185/200\n",
      " - 220s - loss: 0.7701 - acc: 0.7542\n",
      "Epoch 186/200\n",
      " - 218s - loss: 0.7677 - acc: 0.7550\n",
      "Epoch 187/200\n",
      " - 222s - loss: 0.7670 - acc: 0.7548\n",
      "Epoch 188/200\n",
      " - 215s - loss: 0.7657 - acc: 0.7551\n",
      "Epoch 189/200\n",
      " - 213s - loss: 0.7635 - acc: 0.7567\n",
      "Epoch 190/200\n",
      " - 221s - loss: 0.7695 - acc: 0.7541\n",
      "Epoch 191/200\n",
      " - 219s - loss: 0.7655 - acc: 0.7549\n",
      "Epoch 192/200\n",
      " - 219s - loss: 0.7663 - acc: 0.7544\n",
      "Epoch 193/200\n",
      " - 218s - loss: 0.7578 - acc: 0.7567\n",
      "Epoch 194/200\n",
      " - 219s - loss: 0.7597 - acc: 0.7572\n",
      "Epoch 195/200\n",
      " - 219s - loss: 0.7552 - acc: 0.7585\n",
      "Epoch 196/200\n",
      " - 220s - loss: 0.7604 - acc: 0.7570\n",
      "Epoch 197/200\n",
      " - 218s - loss: 0.7549 - acc: 0.7581\n",
      "Epoch 198/200\n",
      " - 219s - loss: 0.7564 - acc: 0.7588\n",
      "Epoch 199/200\n",
      " - 220s - loss: 0.7551 - acc: 0.7579\n",
      "Epoch 200/200\n",
      " - 220s - loss: 0.7566 - acc: 0.7577\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1849ceb38>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(LSTM(200, input_shape=(X.shape[1], X.shape[2])))\n",
    "model3.add(Dropout(0.3))\n",
    "model3.add(Dense(200, input_shape=(X.shape[1], X.shape[2])))\n",
    "model3.add(Dropout(0.3))\n",
    "model3.add(Dense(200, input_shape=(X.shape[1], X.shape[2])))\n",
    "model3.add(Dropout(0.3))\n",
    "\n",
    "# adding temperature\n",
    "temp = 0.25\n",
    "model3.add(Lambda(lambda x : x /temp))\n",
    "model3.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "print(model3.summary())\n",
    "# compile model\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "\n",
    "model3.fit(X, y, epochs=200, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thine own sweet self alone so shall think that time: \n",
      "With the canst not part and what is to let: \n",
      "Me fair lest thy part that I  this is the: \n",
      "Praise the earth a drought to be the grave though, \n",
      "All the less your sweet self alone should be as, \n",
      "I make shall best I  may I  sing when I , \n",
      "Say me then make hap you may I  make my, \n",
      "Love as tell and sake and I  may not remove? \n",
      "Nor me thine in their spile contents to time thou? \n",
      "Wert to this proud again as an and all thing: \n",
      "That thou shalt fair show that which it shall I : \n",
      "Swear means the praise then make the lease thet fair, \n",
      "To thou thy self desire thou with to make me, \n",
      "Ground the fairest in my love and the conceation dide, \n",
      "The parter me to my love may still shine when, \n",
      "I "
     ]
    }
   ],
   "source": [
    "generate_poem(model3, chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alters rook so like a glasted with the lines of, \n",
      "Love my dead to love withing all my body steep, \n",
      "To bear my tongue-tied my best thou wilt for they. \n",
      "Be old sight my love is their hard and so: \n",
      "Thou thy self desire thou wilt for I  may not: \n",
      "Proved and in the right for shame dear love thee, \n",
      "So store thee in the pride the painter with thy? \n",
      "Sweet sight and shadow shall be as thine thou art, \n",
      "As I  think the lears the strength on thy self: \n",
      "To bear what leave the counters an erpoctoor but for, \n",
      "The list and false more receives o chary the worght, \n",
      "That in the present doth that with thee the place. \n",
      "When I  should your self in my love's gain and, \n",
      "Praises of my love to come in the store to, \n",
      "Hide when in the spier with p."
     ]
    }
   ],
   "source": [
    "summers_day_poem(model3, chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
